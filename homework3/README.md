# Завдання №3. Оркестрація процесів обробки даних в Apache Airflow

## Послідовність виконання завдання


1. Створіть директорію dags, в якій зберігатимуться ваші пайплайни. Відкрийте файл airflow/airflow.cfg, знайдіть змінну dags_folder та встановіть значення абсолютного шляху до директорії dags. Увага: шлях має бути обовʼязково абсолютним!
2. У директорії dags створіть файл process_sales.py, у ньому створіть DAG з dag_id=”process_sales”. DAG має складатися з двох тасок:
  - task_id=”extract_data_from_api”, яка трігає першу джобу з завдання 1
  - task_id=”convert_to_avro”, яка трігає другу джобу з завдання 1
3. Кожна з тасок має успішно завершуватись, якщо відповідь від сервера з джобою 201. В протилежному випадку джоба має фейлитись (бути червоною).
4. Пайплайн має процесити дані за 3 дні: з 2022-08-09 по 2022-08-11 (початкова та кінцева дати включаються в проміжок).
5. Пайплайн має стартувати о 1 ночі кожного дня за UTC (використайте cron notation).
6. Якщо dag run запускається за конкретну дату, то дані мають процеситись лише за цю дату.
7. Джоби приймають параметри raw_dir та stg_dir, які є шляхами з датою (наприклад, /path/to/my_dir/raw/sales/2022-08-09). Слідкуйте, щоб для кожного dag run за відповідну дату був правильний шлях.
8. Airflow локально працює в однозадачному режимі (в один час може скедюлитись лише одна таска). Але для підстраховки рекомендується встановити наступний параметр в DAG: max_active_runs=1, щоб запобігти одночасному запуску декількох dag ранів (джоби запускаються локально, а не в реальному середовищі виконання, тому вони не можуть обслуговувати лише один запит одночасно).
9. Також встановіть в DAG параметр: catchup=True. Таким чином, Airflow запустить рани за дні, що в минулому.
10. Рекомендується протестити весь workflow. Для цього необхідно запуcтити першу джобу в одному терміналі, другу джобу в другому терміналі. Airflow потрібно запустити в окремому терміналі. Слідкуйте, щоб в кожному середовищі було активоване відповідне віртуальне середовище Python. Якщо ви все зробили вірно, Airflow запустить dag рани за 3 дні послідовно, бо всі 3 дати вже в минулому. У вашій папці storage мають зʼявитися папки з датами та з даними.
